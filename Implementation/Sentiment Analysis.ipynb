{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO8aQBZTQJoQNORSekjk/gU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvLjEa2oOUxs","executionInfo":{"status":"ok","timestamp":1712288131102,"user_tz":-330,"elapsed":4010,"user":{"displayName":"Ramsai koushik polisetty","userId":"14452151933705663551"}},"outputId":"3d83b91f-4eac-410c-a284-cbfa6e860ce8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"]}],"source":["import nltk\n","nltk.download('twitter_samples')\n","\n","from nltk.corpus import twitter_samples"]},{"cell_type":"code","source":["# Load the positive and negative tweets\n","positive_tweets = twitter_samples.strings('positive_tweets.json')\n","negative_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","print(\"Number of positive tweets:\", len(positive_tweets))\n","print(\"Number of negative tweets:\", len(negative_tweets))\n","\n","\n","print(positive_tweets[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHEIV1Q-OaWE","executionInfo":{"status":"ok","timestamp":1712288135186,"user_tz":-330,"elapsed":2051,"user":{"displayName":"Ramsai koushik polisetty","userId":"14452151933705663551"}},"outputId":"89207093-9cb6-467e-e34f-d3f9f837328f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of positive tweets: 5000\n","Number of negative tweets: 5000\n","#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import nltk\n","import tensorflow\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Download NLTK resources (if not already downloaded)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Tokenization, Stopword Removal, and Stemming\n","def preprocess_tweet(tweet):\n","    # Tokenization\n","    tokens = word_tokenize(tweet)\n","\n","    # Remove noise (non-alphabetic characters)\n","    tokens = [token for token in tokens if token.isalpha()]\n","\n","    # Remove stopwords (retaining negation words)\n","    stop_words = set(stopwords.words('english'))\n","    negation_words = set([\"not\", \"no\", \"never\"])  # Negation words to retain\n","    filtered_tokens = [token for token in tokens if token.lower() not in stop_words or token.lower() in negation_words]\n","\n","    # Stemming\n","    stemmer = PorterStemmer()\n","    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n","\n","    return stemmed_tokens\n","\n","# Combine positive and negative tweets into a single list\n","all_tweets = positive_tweets + negative_tweets\n","\n","# Preprocess tweets\n","preprocessed_tweets = [preprocess_tweet(tweet) for tweet in all_tweets]\n","\n","# Initialize tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(preprocessed_tweets)\n","\n","# Convert text data to sequences of indices\n","sequences = tokenizer.texts_to_sequences(preprocessed_tweets)\n","\n","# Pad sequences to a fixed length\n","\n","# Find the maximum length of sequences\n","max_length = max([len(seq) for seq in sequences])\n","print(\"Maximum sequence length:\", max_length)\n","\n","padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n","\n","# Create labels for positive and negative tweets (1 for positive, 0 for negative)\n","labels = np.concatenate((np.ones(len(positive_tweets)), np.zeros(len(negative_tweets))))\n","\n","# Split data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNKMuWea4N__","executionInfo":{"status":"ok","timestamp":1712288184261,"user_tz":-330,"elapsed":11802,"user":{"displayName":"Ramsai koushik polisetty","userId":"14452151933705663551"}},"outputId":"ab57f6f8-d243-4e5c-d0f5-8347614d0416"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 28\n"]}]},{"cell_type":"code","source":["print(padded_sequences[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHHc3aqSWg3w","executionInfo":{"status":"ok","timestamp":1712288578901,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ramsai koushik polisetty","userId":"14452151933705663551"}},"outputId":"eaaf1180-27ad-49cb-f966-0e65a6840452"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 375  256 1048  410  266   50    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Define the RNN model\n","def create_rnn_model(input_dim, output_dim, embedding_dim=128, rnn_units=64):\n","    model = Sequential()\n","    model.add(Embedding(input_dim, embedding_dim, input_length=max_length))\n","    model.add(SimpleRNN(rnn_units))\n","    model.add(Dense(1, activation='sigmoid'))\n","    return model\n","\n","# Create the RNN model\n","model = create_rnn_model(input_dim=len(tokenizer.word_index) + 1, output_dim=1)\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIj4vrwgP8-a","executionInfo":{"status":"ok","timestamp":1712288289613,"user_tz":-330,"elapsed":23150,"user":{"displayName":"Ramsai koushik polisetty","userId":"14452151933705663551"}},"outputId":"4b6ca809-12b7-451e-894c-96d70adc4549"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","113/113 [==============================] - 7s 40ms/step - loss: 0.5941 - accuracy: 0.6711 - val_loss: 0.5022 - val_accuracy: 0.7750\n","Epoch 2/5\n","113/113 [==============================] - 4s 37ms/step - loss: 0.2452 - accuracy: 0.9056 - val_loss: 0.5767 - val_accuracy: 0.7225\n","Epoch 3/5\n","113/113 [==============================] - 4s 38ms/step - loss: 0.1039 - accuracy: 0.9651 - val_loss: 0.6470 - val_accuracy: 0.7462\n","Epoch 4/5\n","113/113 [==============================] - 4s 34ms/step - loss: 0.0623 - accuracy: 0.9790 - val_loss: 0.7347 - val_accuracy: 0.7300\n","Epoch 5/5\n","113/113 [==============================] - 4s 37ms/step - loss: 0.0462 - accuracy: 0.9817 - val_loss: 0.7142 - val_accuracy: 0.7625\n"]}]},{"cell_type":"code","source":["# Evaluate the model\n","y_pred_prob = model.predict(X_test)\n","y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwUs3jnqQMeG","executionInfo":{"status":"ok","timestamp":1712288297014,"user_tz":-330,"elapsed":1024,"user":{"displayName":"Ramsai koushik polisetty","userId":"14452151933705663551"}},"outputId":"79cf4f98-437f-4eb9-e8f5-eb17b1e9d949"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["63/63 [==============================] - 0s 4ms/step\n","Accuracy: 0.721\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.72      0.72      0.72       988\n","         1.0       0.73      0.72      0.72      1012\n","\n","    accuracy                           0.72      2000\n","   macro avg       0.72      0.72      0.72      2000\n","weighted avg       0.72      0.72      0.72      2000\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0MQRjWhsRJo4"},"execution_count":null,"outputs":[]}]}